{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Введение в NLP‑эмбеддинги на PyTorch: TF‑IDF и Word2Vec (Disaster Tweets)\n",
        "\n",
        "В этом практическом ноутбуке по шагам разберём:\n",
        "- что такое эмбеддинги текста и зачем они нужны;\n",
        "- как посчитать TF‑IDF и интерпретировать веса;\n",
        "- как обучить Word2Vec (CBOW и Skip‑Gram) на PyTorch без классов и функций — максимально линейный, читаемый код;\n",
        "- как визуализировать векторные представления слов и документов c помощью Plotly (PCA/t‑SNE);\n",
        "- как сравнить TF‑IDF и Word2Vec и сделать выводы.\n",
        "\n",
        "Датасет: Kaggle `Disaster Tweets` (`vstepanenko/disaster-tweets`). Если локально есть `tweets.csv`, используем его. В противном случае — инструкция по ручной загрузке с Kaggle.\n",
        "\n",
        "Полезные ссылки:\n",
        "- Страница датасета — `https://www.kaggle.com/datasets/vstepanenko/disaster-tweets`\n",
        "- TF‑IDF (sklearn) — `https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction`\n",
        "- Word Embeddings Tutorial (PyTorch) — `https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html`\n",
        "- Word2Vec (Mikolov et al.) — `https://arxiv.org/abs/1301.3781`, `https://arxiv.org/abs/1310.4546`\n",
        "- Plotly — `https://plotly.com/python/`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка/импорты, сиды, device\n",
        "import os, random, re, string, math, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import SGD\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import plotly.express as px\n",
        "from collections import Counter\n",
        "\n",
        "# Сиды для воспроизводимости\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Устройство\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "pd.set_option('display.max_colwidth', 120)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Датасеты и выбор Disaster Tweets\n",
        "\n",
        "`Disaster Tweets` — короткие тексты твитов с бинарной меткой `target` о наличии признаков катастрофы. Формат удобен для построения базовых представлений (TF‑IDF, Word2Vec) и интерактивной визуализации.\n",
        "\n",
        "Другие корпуса:\n",
        "- IMDB (рецензии, бинарная сентимент‑классификация)\n",
        "- SMS Spam (спам/не‑спам)\n",
        "- AG News (4‑классовые новости)\n",
        "- BBC News (тематики новостей)\n",
        "- TREC‑6 (тип вопроса)\n",
        "\n",
        "Для воспроизведения требуется таблица со столбцами `text`, `target`. При отсутствии `tweets.csv` загрузите с Kaggle: `https://www.kaggle.com/datasets/vstepanenko/disaster-tweets`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка датасета: пытаемся прочитать локальный tweets.csv, иначе даём подсказку\n",
        "csv_path = \"tweets.csv\"\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "else:\n",
        "    print(\"Файл tweets.csv не найден. Скачайте с Kaggle и поместите в корень проекта:\")\n",
        "    print(\"https://www.kaggle.com/datasets/vstepanenko/disaster-tweets\")\n",
        "    print(\"Ожидаемые столбцы: text, target\")\n",
        "    # создаём маленький демо‑датасет для отладки структуры ноутбука\n",
        "    df = pd.DataFrame({\n",
        "        \"text\": [\n",
        "            \"Fire in the street near river.\",\n",
        "            \"Good morning everyone!\",\n",
        "            \"Earthquake reported yesterday.\",\n",
        "            \"I love sunny days and coffee.\",\n",
        "            \"Flood alert in the city center!\"\n",
        "        ],\n",
        "        \"target\": [1, 0, 1, 0, 1]\n",
        "    })\n",
        "\n",
        "df = df[[\"text\", \"target\"]].dropna()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Предобработка текста\n",
        "\n",
        "Шаги:\n",
        "- приведение к нижнему регистру;\n",
        "- замена ссылок на токен `<LINK>` и упоминаний на `<MENTION>`;\n",
        "- удаление пунктуации и цифр; сжатие пробелов;\n",
        "- токенизация через `.split()` (при необходимости можно заменить на `nltk`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# регулярные выражения для токенов ссылок и упоминаний\n",
        "url_re = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "mention_re = re.compile(r\"@[A-Za-z0-9_]+\")\n",
        "# хэштеги не удаляются отдельной регул. замены; символ # будет удалён при удалении пунктуации\n",
        "number_re = re.compile(r\"\\d+\")\n",
        "punct_table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "# исходные тексты и целевые метки\n",
        "texts = df[\"text\"].astype(str).tolist()\n",
        "labels = df[\"target\"].astype(int).tolist()\n",
        "\n",
        "# очистка и нормализация текста\n",
        "clean_texts = []\n",
        "for t in texts:\n",
        "    t = t.lower()                                              # нижний регистр\n",
        "    t = url_re.sub(\" <LINK> \", t)                             # ссылки → токен <LINK>\n",
        "    t = mention_re.sub(\" <MENTION> \", t)                      # упоминания → токен <MENTION>\n",
        "    t = number_re.sub(\" \", t)                                 # удаление цифр\n",
        "    t = t.translate(punct_table)                                # удаление пунктуации\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()                      # схлопывание пробелов\n",
        "    clean_texts.append(t)\n",
        "\n",
        "# токенизация по пробелам\n",
        "tokenized = [t.split() for t in clean_texts]\n",
        "\n",
        "# разбиение на обучающую/валидационную части\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(\n",
        "    clean_texts, labels, test_size=0.2, random_state=seed, stratify=labels if len(set(labels))>1 else None\n",
        ")\n",
        "\n",
        "# контрольный вывод\n",
        "print(\"Пример очищенного текста:\", clean_texts[0])\n",
        "print(\"Размеры:\", len(train_texts), len(valid_texts))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF‑IDF: идея и формулы\n",
        "\n",
        "TF — частота термина в документе. IDF — обратная документная частота, уменьшающая вес часто встречающихся слов.\n",
        "\n",
        "Формулы (с учётом сглаживания в sklearn):\n",
        "- tf — относительная частота в документе;\n",
        "- idf = log((N + 1) / (df + 1)) + 1, где N — число документов, df — в скольких документах слово встречается;\n",
        "- tfidf = tf * idf.\n",
        "\n",
        "Далее посчитаем TF‑IDF для обучающей части корпуса, преобразуем в `torch.tensor` и покажем простые операции (нормы, косинусная близость).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_features = 3000\n",
        "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1,1))\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_valid = vectorizer.transform(valid_texts)\n",
        "\n",
        "print(\"TF‑IDF форма:\", X_train.shape)\n",
        "\n",
        "# Перевод в torch\n",
        "tf_train = torch.tensor(X_train.toarray(), dtype=torch.float32, device=device)\n",
        "tf_valid = torch.tensor(X_valid.toarray(), dtype=torch.float32, device=device)\n",
        "\n",
        "# Косинусная близость между первыми двумя документами\n",
        "u = tf_train[0]\n",
        "v = tf_train[1] if tf_train.shape[0] > 1 else tf_train[0]\n",
        "cos_sim = torch.nn.functional.cosine_similarity(u.unsqueeze(0), v.unsqueeze(0)).item()\n",
        "print(\"Косинусная близость первых двух документов:\", round(cos_sim, 4))\n",
        "\n",
        "vocab_tfidf = vectorizer.get_feature_names_out()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализации TF‑IDF: документы в 2D (PCA) и топ‑слова\n",
        "\n",
        "# Документы в 2D по PCA\n",
        "pca_docs = PCA(n_components=2, random_state=seed)\n",
        "coords = pca_docs.fit_transform(X_train.toarray())\n",
        "fig = px.scatter(x=coords[:,0], y=coords[:,1], color=pd.Series(train_labels, name=\"target\"),\n",
        "                 title=\"Документы (TF‑IDF) в проекции PCA\")\n",
        "fig.show()\n",
        "\n",
        "# Топ‑слова по среднему TF‑IDF в положительном/отрицательном классе\n",
        "train_df = pd.DataFrame({\"text\": train_texts, \"target\": train_labels})\n",
        "X_all = vectorizer.transform(train_df[\"text\"]).toarray()\n",
        "mean_pos = X_all[np.array(train_df[\"target\"])==1].mean(axis=0) if (np.array(train_df[\"target\"])==1).any() else np.zeros(X_all.shape[1])\n",
        "mean_neg = X_all[np.array(train_df[\"target\"])==0].mean(axis=0) if (np.array(train_df[\"target\"])==0).any() else np.zeros(X_all.shape[1])\n",
        "\n",
        "k = 15\n",
        "top_pos_idx = np.argsort(-mean_pos)[:k]\n",
        "top_neg_idx = np.argsort(-mean_neg)[:k]\n",
        "\n",
        "fig1 = px.bar(x=vocab_tfidf[top_pos_idx], y=mean_pos[top_pos_idx], title=\"Топ‑слова TF‑IDF: target=1\")\n",
        "fig2 = px.bar(x=vocab_tfidf[top_neg_idx], y=mean_neg[top_neg_idx], title=\"Топ‑слова TF‑IDF: target=0\")\n",
        "fig1.show(); fig2.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Подготовка словаря и данных для Word2Vec (CBOW/Skip‑Gram)\n",
        "\n",
        "Сделаем простой словарь по частотам, добавим `PAD` и `UNK`, отфильтруем редкие слова. Затем создадим пары для CBOW и Skip‑Gram одним линейным кодом с окном контекста.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Строим словарь и индексируем тексты\n",
        "min_freq = 2\n",
        "counter = Counter(w for t in tokenized for w in t)\n",
        "# базовые токены\n",
        "itos = [\"<PAD>\", \"<UNK>\"] + [w for w, c in counter.items() if c >= min_freq]\n",
        "stoi = {w: i for i, w in enumerate(itos)}\n",
        "\n",
        "# индексируем тексты\n",
        "token_ids = []\n",
        "for t in tokenized:\n",
        "    ids = [stoi.get(w, 1) for w in t]  # 1 == <UNK>\n",
        "    if len(ids) > 0:\n",
        "        token_ids.append(ids)\n",
        "\n",
        "# параметры словаря и размер эмбеддинга\n",
        "vocab_size = len(itos)\n",
        "emb_dim = 100\n",
        "print(\"Словарь:\", vocab_size, \"Документов:\", len(token_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Генерация пар для CBOW и Skip‑Gram\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "window = 2\n",
        "cbow_pairs = []   # (context_ids, target_id)\n",
        "skip_pairs = []   # (target_id, context_id)\n",
        "for ids in tqdm(token_ids, desc=\"Генерация пар\"):\n",
        "    L = len(ids)\n",
        "    for i in range(L):\n",
        "        left = max(0, i - window)\n",
        "        right = min(L, i + window + 1)\n",
        "        context = [ids[j] for j in range(left, right) if j != i]\n",
        "        if len(context) == 0:\n",
        "            continue\n",
        "        cbow_pairs.append((context, ids[i]))\n",
        "        for c in context:\n",
        "            skip_pairs.append((ids[i], c))\n",
        "\n",
        "# Ограничим количество для демонстрации\n",
        "max_pairs = 5000\n",
        "cbow_pairs = cbow_pairs[:max_pairs]\n",
        "skip_pairs = skip_pairs[:max_pairs]\n",
        "\n",
        "print(\"CBOW пар:\", len(cbow_pairs), \"SG пар:\", len(skip_pairs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2Vec CBOW (полный софтмакс)\n",
        "\n",
        "Идея: по контексту предсказываем центральное слово. Архитектура:\n",
        "- `Embedding(vocab_size, emb_dim)` для слов;\n",
        "- усреднение контекстных векторов;\n",
        "- `Linear(emb_dim → vocab_size)` и `CrossEntropyLoss` (эквивалентно `LogSoftmax + NLLLoss`).\n",
        "\n",
        "Функции активации и потерь:\n",
        "- На выходе линейного слоя получаем логиты. `CrossEntropyLoss` внутри применяет `LogSoftmax`, поэтому вручную `Softmax` не нужен.\n",
        "- Интерпретация: `Softmax` превращает логиты в вероятности по словарю; `CrossEntropy` наказывает за неправильный индекс целевого слова.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Что такое `nn.Embedding` в PyTorch\n",
        "\n",
        "`nn.Embedding` — это обучаемая таблица соответствий индексов и векторов признаков (lookup table).\n",
        "Фактически это матрица весов `W ∈ R[num_embeddings × embedding_dim]`, где каждая строка — вектор слова/токена.\n",
        "Входом служат целочисленные индексы (`LongTensor`) формы `[..., sequence_len]`, на выходе — вещественные векторы формы `[..., sequence_len, embedding_dim]`.\n",
        "\n",
        "Ключевые параметры:\n",
        "- `num_embeddings`: размер словаря (число токенов);\n",
        "- `embedding_dim`: размерность эмбеддинга;\n",
        "- `padding_idx` (опц.): индекс токена‑паддинга; соответствующая строка не обучается и всегда остаётся нулевой;\n",
        "- `max_norm`, `norm_type` (опц.): ограничение нормы строк при обновлении;\n",
        "- `scale_grad_by_freq` (опц.): масштабирование градиента по частоте слова;\n",
        "- `sparse` (опц.): разрежённые градиенты для ускорения при больших словарях.\n",
        "\n",
        "Как это работает:\n",
        "- Эквивалент умножению one‑hot представления на матрицу `W`, но без явного создания one‑hot; извлекаются нужные строки `W` по индексам.\n",
        "- Градиенты протекают только через те строки, которые были использованы на данном батче.\n",
        "- Слой не применяет смещения или нелинейности: это чистый lookup, поэтому в CBOW/Skip‑Gram поверх усреднённого/взятого эмбеддинга добавляется `Linear` и уже затем считается `CrossEntropyLoss`.\n",
        "\n",
        "В этом ноутбуке:\n",
        "- В CBOW берём эмбеддинги контекстных слов и усредняем; результат подаётся в `Linear(embedding_dim → vocab_size)`.\n",
        "- В Skip‑Gram эмбеддинг центрального слова подаётся в `Linear(embedding_dim → vocab_size)` для предсказания контекстного слова.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Простой CBOW без классов: просто объявляем слои и обучаем\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "cbow_embed = nn.Embedding(vocab_size, emb_dim).to(device)\n",
        "cbow_linear = nn.Linear(emb_dim, vocab_size).to(device)\n",
        "\n",
        "cbow_loss = nn.CrossEntropyLoss()\n",
        "cbow_opt = SGD(list(cbow_embed.parameters()) + list(cbow_linear.parameters()), lr=0.1)\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for ctx_ids, tgt_id in tqdm(cbow_pairs, desc=f\"CBOW epoch {epoch+1}\"):\n",
        "        ctx_tensor = torch.tensor(ctx_ids, dtype=torch.long, device=device)\n",
        "        tgt_tensor = torch.tensor([tgt_id], dtype=torch.long, device=device)\n",
        "\n",
        "        # прямой проход\n",
        "        ctx_vecs = cbow_embed(ctx_tensor)          # [context_len, emb_dim]\n",
        "        ctx_mean = ctx_vecs.mean(dim=0, keepdim=True)  # [1, emb_dim]\n",
        "        logits = cbow_linear(ctx_mean)             # [1, vocab_size]\n",
        "\n",
        "        loss = cbow_loss(logits, tgt_tensor)\n",
        "\n",
        "        cbow_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        cbow_opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"CBOW epoch {epoch+1}: loss={total_loss:.2f}\")\n",
        "\n",
        "# Сохраняем обученные эмбеддинги слов (матрица)\n",
        "embeddings_cbow = cbow_embed.weight.detach().cpu().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2Vec Skip‑Gram (полный софтмакс)\n",
        "\n",
        "Идея: по центральному слову предсказываем каждое слово из его контекста. Архитектура аналогична:\n",
        "- `Embedding(vocab_size, emb_dim)`;\n",
        "- `Linear(emb_dim → vocab_size)`;\n",
        "- `CrossEntropyLoss`.\n",
        "\n",
        "Замечание: для ускорения в реальных проектах применяют Negative Sampling и `BCEWithLogitsLoss`, но здесь используем полный софтмакс ради простоты и прозрачности.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Простой Skip‑Gram без классов\n",
        "sg_embed = nn.Embedding(vocab_size, emb_dim).to(device)\n",
        "sg_linear = nn.Linear(emb_dim, vocab_size).to(device)\n",
        "\n",
        "sg_loss = nn.CrossEntropyLoss()\n",
        "sg_opt = SGD(list(sg_embed.parameters()) + list(sg_linear.parameters()), lr=0.1)\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for tgt_id, ctx_id in skip_pairs:\n",
        "        tgt_tensor = torch.tensor([tgt_id], dtype=torch.long, device=device)\n",
        "        ctx_tensor = torch.tensor([ctx_id], dtype=torch.long, device=device)\n",
        "\n",
        "        tgt_vec = sg_embed(tgt_tensor)      # [1, emb_dim]\n",
        "        logits = sg_linear(tgt_vec)         # [1, vocab_size]\n",
        "\n",
        "        loss = sg_loss(logits, ctx_tensor)\n",
        "\n",
        "        sg_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        sg_opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Skip‑Gram epoch {epoch+1}: loss={total_loss:.2f}\")\n",
        "\n",
        "embeddings_sg = sg_embed.weight.detach().cpu().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Визуализация эмбеддингов слов (PCA / t‑SNE)\n",
        "\n",
        "Сравним распределения слов для CBOW и Skip‑Gram. Для наглядности возьмём топ‑N слов по частоте.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Выберем топ‑N частых слов (исключая спец‑токены)\n",
        "N = 300\n",
        "freq_sorted = sorted([(w, c) for w, c in counter.items() if w in stoi], key=lambda x: -x[1])\n",
        "words = [w for w, _ in freq_sorted if w not in (\"<PAD>\", \"<UNK>\")][:N]\n",
        "idxs = [stoi[w] for w in words]\n",
        "\n",
        "sub_cbow = embeddings_cbow[idxs]\n",
        "sub_sg = embeddings_sg[idxs]\n",
        "\n",
        "# PCA до 2D\n",
        "pca2 = PCA(n_components=2, random_state=seed)\n",
        "coords_cbow = pca2.fit_transform(sub_cbow)\n",
        "coords_sg = pca2.fit_transform(sub_sg)\n",
        "\n",
        "fig = px.scatter(x=coords_cbow[:,0], y=coords_cbow[:,1], text=words, title=\"CBOW: слова в 2D (PCA)\")\n",
        "fig.update_traces(textposition='top center', marker=dict(size=6))\n",
        "fig.show()\n",
        "\n",
        "fig = px.scatter(x=coords_sg[:,0], y=coords_sg[:,1], text=words, title=\"Skip‑Gram: слова в 2D (PCA)\")\n",
        "fig.update_traces(textposition='top center', marker=dict(size=6))\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ближайшие слова по косинусной близости\n",
        "\n",
        "Найдём ближайшие слова для нескольких запросов (например: `fire`, `earthquake`, `flood`, `help`) по эмбеддингам CBOW и Skip‑Gram.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nearest_words(query_word, emb_matrix, k=8):\n",
        "    if query_word not in stoi:\n",
        "        return []\n",
        "    q_idx = stoi[query_word]\n",
        "    qv = emb_matrix[q_idx]\n",
        "    M = emb_matrix\n",
        "    # косинусная близость вручную\n",
        "    num = (M @ qv)\n",
        "    denom = (np.linalg.norm(M, axis=1) * (np.linalg.norm(qv) + 1e-9) + 1e-9)\n",
        "    cos = num / denom\n",
        "    order = np.argsort(-cos)\n",
        "    result = []\n",
        "    for idx in order[:k+1]:  # +1 чтобы включить сам запрос, потом отфильтруем\n",
        "        w = itos[idx] if idx < len(itos) else None\n",
        "        if w is None or w in (\"<PAD>\", \"<UNK>\") or w == query_word:\n",
        "            continue\n",
        "        result.append((w, float(cos[idx])))\n",
        "        if len(result) == k:\n",
        "            break\n",
        "    return result\n",
        "\n",
        "for word in [\"fire\", \"earthquake\", \"flood\", \"help\"]:\n",
        "    print(\"\\nСлово:\", word)\n",
        "    print(\"CBOW:\", nearest_words(word, embeddings_cbow, k=8))\n",
        "    print(\"SG:  \", nearest_words(word, embeddings_sg, k=8))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Документные эмбеддинги и сравнение с TF‑IDF\n",
        "\n",
        "Простой способ получить вектор документа на базе словарных эмбеддингов — среднее по словам (можно также взвешивать по TF‑IDF). Сравним распределения документов, полученных по CBOW/SG, с проекцией TF‑IDF.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# усреднение эмбеддингов слов в документе\n",
        "\n",
        "# подготовим индексированные документы для train_texts (как после очистки)\n",
        "train_tokens = [t.split() for t in train_texts]\n",
        "train_ids = [[stoi.get(w, 1) for w in doc if w in stoi] for doc in train_tokens]\n",
        "\n",
        "# среднее по CBOW\n",
        "doc_emb_cbow = []\n",
        "for ids in train_ids:\n",
        "    if len(ids) == 0:\n",
        "        doc_emb_cbow.append(np.zeros(emb_dim))\n",
        "    else:\n",
        "        doc_emb_cbow.append(embeddings_cbow[ids].mean(axis=0))\n",
        "doc_emb_cbow = np.stack(doc_emb_cbow)\n",
        "\n",
        "# среднее по SG\n",
        "doc_emb_sg = []\n",
        "for ids in train_ids:\n",
        "    if len(ids) == 0:\n",
        "        doc_emb_sg.append(np.zeros(emb_dim))\n",
        "    else:\n",
        "        doc_emb_sg.append(embeddings_sg[ids].mean(axis=0))\n",
        "doc_emb_sg = np.stack(doc_emb_sg)\n",
        "\n",
        "# PCA до 2D и визуализация\n",
        "pca_docs2 = PCA(n_components=2, random_state=seed)\n",
        "coords_cbow_docs = pca_docs2.fit_transform(doc_emb_cbow)\n",
        "fig = px.scatter(x=coords_cbow_docs[:,0], y=coords_cbow_docs[:,1], color=pd.Series(train_labels, name=\"target\"),\n",
        "                 title=\"Документы: среднее словарных эмбеддингов (CBOW) → PCA\")\n",
        "fig.show()\n",
        "\n",
        "coords_sg_docs = pca_docs2.fit_transform(doc_emb_sg)\n",
        "fig = px.scatter(x=coords_sg_docs[:,0], y=coords_sg_docs[:,1], color=pd.Series(train_labels, name=\"target\"),\n",
        "                 title=\"Документы: среднее словарных эмбеддингов (SG) → PCA\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Выводы\n",
        "\n",
        "- **TF‑IDF** хорошо работает как базовый метод: просто, быстро, интерпретируемо. Он учитывает важность слов через IDF, но не понимает семантику и порядок слов.\n",
        "- **Word2Vec (CBOW/Skip‑Gram)** учится на контекстах и лучше улавливает семантическую близость слов. CBOW обычно быстрее и устойчивее на маленьких данных, Skip‑Gram — лучше для редких слов.\n",
        "- **Визуализации** помогают увидеть кластеры и интуитивно понять структуру пространства эмбеддингов.\n",
        "- Для продвинутых задач: попробуйте `negative sampling` (BCEWithLogitsLoss), `fastText` (учёт субслов), предобученные векторы (`GloVe`), трансформеры (`BERT` и др.).\n",
        "\n",
        "Полезные ссылки:\n",
        "- Kaggle Disaster Tweets — `https://www.kaggle.com/datasets/vstepanenko/disaster-tweets`\n",
        "- PyTorch Word Embeddings Tutorial — `https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html`\n",
        "- TF‑IDF (sklearn) — `https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction`\n",
        "- Word2Vec: Mikolov et al. — `https://arxiv.org/abs/1301.3781`, `https://arxiv.org/abs/1310.4546`\n",
        "- Plotly — `https://plotly.com/python/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Как это будет на предобученных эмбеддингах (EmbeddingGemma‑300M)\n",
        "\n",
        "Используются эмбеддинги из модели `google/embeddinggemma-300m` (Hugging Face). Размер вектора по умолчанию — 768. Для демонстрации: извлечение эмбеддингов для отдельных твитов, визуализация в 2D и поиск ближайших соседей.\n",
        "\n",
        "Ссылки:\n",
        "- Модель: `https://huggingface.co/google/embeddinggemma-300m`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!uv add sentence-transformers hf_xet\n",
        "!pip install sentence-transformers hf_xet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# установка и импорт модели эмбеддингов\n",
        "# !pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# загрузка предобученной модели (использует float32/bfloat16)\n",
        "embed_model = SentenceTransformer(\"google/embeddinggemma-300m\", device=str(device))\n",
        "\n",
        "# --- Визуализация эмбеддингов отдельных слов (топ-N по частоте) ---\n",
        "N_words = 200\n",
        "words_sorted = sorted(counter.items(), key=lambda x: -x[1])\n",
        "words_for_gemma = [w for w, c in words_sorted if w and w not in (\"<PAD>\", \"<UNK>\")][:N_words]\n",
        "\n",
        "emb_words = embed_model.encode(words_for_gemma, convert_to_tensor=True, device=str(device))\n",
        "emb_words = emb_words.detach().cpu().numpy()\n",
        "\n",
        "pca_w = PCA(n_components=2, random_state=seed)\n",
        "coords_w = pca_w.fit_transform(emb_words)\n",
        "df_words = pd.DataFrame({\"x\": coords_w[:,0], \"y\": coords_w[:,1], \"word\": words_for_gemma})\n",
        "fig = px.scatter(df_words, x=\"x\", y=\"y\", text=\"word\", hover_name=\"word\", title=\"EmbeddingGemma‑300M: слова → PCA\")\n",
        "fig.update_traces(textposition=\"top center\", marker=dict(size=6))\n",
        "fig.show()\n",
        "\n",
        "# --- Документные эмбеддинги с подсказкой исходного текста ---\n",
        "# выбор подмножества текстов для демонстрации\n",
        "sample_texts = train_texts[:500] if len(train_texts) > 500 else train_texts\n",
        "\n",
        "# извлечение эмбеддингов документов\n",
        "# Важно: использовать encode_document для документов, encode_query для запросов (если нужно сравнение запрос‑документ)\n",
        "emb_gemma = embed_model.encode(sample_texts, convert_to_tensor=True, device=str(device))\n",
        "emb_gemma = emb_gemma.detach().cpu().numpy()\n",
        "\n",
        "# визуализация в 2D с подсказкой-оригиналом\n",
        "pca_g = PCA(n_components=2, random_state=seed)\n",
        "coords_g = pca_g.fit_transform(emb_gemma)\n",
        "df_docs = pd.DataFrame({\n",
        "    \"x\": coords_g[:,0],\n",
        "    \"y\": coords_g[:,1],\n",
        "    \"target\": pd.Series(train_labels[:len(sample_texts)], name=\"target\"),\n",
        "    \"text\": sample_texts\n",
        "})\n",
        "fig = px.scatter(df_docs, x=\"x\", y=\"y\", color=\"target\", hover_name=\"text\", title=\"Документы: EmbeddingGemma‑300M → PCA\")\n",
        "fig.show()\n",
        "\n",
        "# пример ближайших соседей среди документов по косинусной близости\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "if len(sample_texts) >= 5:\n",
        "    sims = cosine_similarity(emb_gemma)\n",
        "    anchor = 0\n",
        "    order = np.argsort(-sims[anchor])\n",
        "    print(\"Пример документа:\", sample_texts[anchor][:200])\n",
        "    print(\"Ближайшие документы:\")\n",
        "    for idx in order[1:6]:\n",
        "        print(\"\\t\", idx, round(float(sims[anchor, idx]), 3), sample_texts[idx][:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Классификация текста на документах: сравнение представлений (PyTorch)\n",
        "\n",
        "Сравниваются три представления документа:\n",
        "- среднее словарных эмбеддингов CBOW;\n",
        "- среднее словарных эмбеддингов Skip‑Gram;\n",
        "- EmbeddingGemma‑300M документные эмбеддинги (классификация в отдельной ячейке ниже).\n",
        "\n",
        "Модель классификации: линейный слой (`Linear(d, 1)`) и `BCEWithLogitsLoss`.\n",
        "- Логиты преобразуются в вероятность через `Sigmoid` на инференсе;\n",
        "- Порог классификации 0.5;\n",
        "- Оптимизатор `Adam`;\n",
        "- Прогресс тренировки — `tqdm`.\n",
        "\n",
        "Метрики: точность и матрица ошибок (confusion matrix) на валидации.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# подготовка документных эмбеддингов для train/valid\n",
        "# TF‑IDF уже разделён; здесь считаем на Word2Vec\n",
        "\n",
        "# CBOW среднее\n",
        "def doc_mean_from_ids(ids_list, emb_matrix, emb_dim):\n",
        "    out = []\n",
        "    for ids in ids_list:\n",
        "        if len(ids) == 0:\n",
        "            out.append(np.zeros(emb_dim))\n",
        "        else:\n",
        "            out.append(emb_matrix[ids].mean(axis=0))\n",
        "    return np.stack(out)\n",
        "\n",
        "# индексы для train/valid\n",
        "train_tokens = [t.split() for t in train_texts]\n",
        "valid_tokens = [t.split() for t in valid_texts]\n",
        "train_ids = [[stoi.get(w, 1) for w in doc if w in stoi] for doc in train_tokens]\n",
        "valid_ids = [[stoi.get(w, 1) for w in doc if w in stoi] for doc in valid_tokens]\n",
        "\n",
        "# документные векторы на основе средних словарных эмбеддингов\n",
        "doc_cbow_train = doc_mean_from_ids(train_ids, embeddings_cbow, emb_dim)\n",
        "doc_cbow_valid = doc_mean_from_ids(valid_ids, embeddings_cbow, emb_dim)\n",
        "\n",
        "doc_sg_train = doc_mean_from_ids(train_ids, embeddings_sg, emb_dim)\n",
        "doc_sg_valid = doc_mean_from_ids(valid_ids, embeddings_sg, emb_dim)\n",
        "\n",
        "# --- Классификация на PyTorch (Linear + BCEWithLogitsLoss) ---\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# подготовка тензоров\n",
        "y_train_t = torch.tensor(train_labels, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "y_valid_t = torch.tensor(valid_labels, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "\n",
        "# параметры обучения\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "lr = 1e-3\n",
        "\n",
        "# 1) CBOW документные векторы\n",
        "Xtr = torch.tensor(doc_cbow_train, dtype=torch.float32, device=device)\n",
        "Xva = torch.tensor(doc_cbow_valid, dtype=torch.float32, device=device)\n",
        "model_cbow = nn.Linear(Xtr.shape[1], 1).to(device)\n",
        "opt_cbow = torch.optim.Adam(model_cbow.parameters(), lr=lr)\n",
        "crit = nn.BCEWithLogitsLoss()\n",
        "for epoch in range(epochs):\n",
        "    model_cbow.train()\n",
        "    total_loss = 0.0\n",
        "    for start in tqdm(range(0, Xtr.shape[0], batch_size), desc=f\"CBOW epoch {epoch+1}\"):\n",
        "        end = start + batch_size\n",
        "        xb = Xtr[start:end]\n",
        "        yb = y_train_t[start:end]\n",
        "        logits = model_cbow(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        opt_cbow.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_cbow.step()\n",
        "        total_loss += float(loss.item())\n",
        "# валидация\n",
        "model_cbow.eval()\n",
        "with torch.no_grad():\n",
        "    val_logits = model_cbow(Xva)\n",
        "    val_prob = torch.sigmoid(val_logits)\n",
        "    val_pred = (val_prob >= 0.5).float()\n",
        "    acc_cbow = float((val_pred.eq(y_valid_t)).float().mean().item())\n",
        "print(\"CBOW val acc:\", round(acc_cbow, 3))\n",
        "\n",
        "# отчёт и матрица ошибок (CBOW)\n",
        "y_true = y_valid_t.detach().cpu().numpy().ravel().astype(int)\n",
        "y_hat = val_pred.detach().cpu().numpy().ravel().astype(int)\n",
        "print(\"CBOW confusion matrix:\\n\", confusion_matrix(y_true, y_hat))\n",
        "print(\"CBOW classification report:\\n\", classification_report(y_true, y_hat, digits=3))\n",
        "\n",
        "# 2) Skip‑Gram документные векторы\n",
        "Xtr = torch.tensor(doc_sg_train, dtype=torch.float32, device=device)\n",
        "Xva = torch.tensor(doc_sg_valid, dtype=torch.float32, device=device)\n",
        "model_sg = nn.Linear(Xtr.shape[1], 1).to(device)\n",
        "opt_sg = torch.optim.Adam(model_sg.parameters(), lr=lr)\n",
        "for epoch in range(epochs):\n",
        "    model_sg.train()\n",
        "    total_loss = 0.0\n",
        "    for start in tqdm(range(0, Xtr.shape[0], batch_size), desc=f\"SG epoch {epoch+1}\"):\n",
        "        end = start + batch_size\n",
        "        xb = Xtr[start:end]\n",
        "        yb = y_train_t[start:end]\n",
        "        logits = model_sg(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        opt_sg.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_sg.step()\n",
        "        total_loss += float(loss.item())\n",
        "model_sg.eval()\n",
        "with torch.no_grad():\n",
        "    val_logits = model_sg(Xva)\n",
        "    val_prob = torch.sigmoid(val_logits)\n",
        "    val_pred = (val_prob >= 0.5).float()\n",
        "    acc_sg = float((val_pred.eq(y_valid_t)).float().mean().item())\n",
        "print(\"SG val acc:\", round(acc_sg, 3))\n",
        "\n",
        "# отчёт и матрица ошибок (SG)\n",
        "y_true = y_valid_t.detach().cpu().numpy().ravel().astype(int)\n",
        "y_hat = val_pred.detach().cpu().numpy().ravel().astype(int)\n",
        "print(\"SG confusion matrix:\\n\", confusion_matrix(y_true, y_hat))\n",
        "print(\"SG classification report:\\n\", classification_report(y_true, y_hat, digits=3))\n",
        "\n",
        "print({\"CBOW\": round(acc_cbow, 3), \"SkipGram\": round(acc_sg, 3)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Классификация на EmbeddingGemma‑300M (PyTorch)\n",
        "\n",
        "Обучение линейного классификатора на документных эмбеддингах EmbeddingGemma‑300M (без функций/классов), `BCEWithLogitsLoss`, `Adam`, `tqdm`. Вывод: точность и матрица ошибок.\n",
        "\n",
        "Ссылка на модель: `https://huggingface.co/google/embeddinggemma-300m`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Документные эмбеддинги EmbeddingGemma (train/valid)\n",
        "emb_gemma_train = embed_model.encode(train_texts, convert_to_tensor=True, device=str(device)).detach().cpu().numpy()\n",
        "emb_gemma_valid = embed_model.encode(valid_texts, convert_to_tensor=True, device=str(device)).detach().cpu().numpy()\n",
        "\n",
        "# Классификация на PyTorch\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "Xtr = torch.tensor(emb_gemma_train, dtype=torch.float32, device=device)\n",
        "Xva = torch.tensor(emb_gemma_valid, dtype=torch.float32, device=device)\n",
        "y_train_t = torch.tensor(train_labels, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "y_valid_t = torch.tensor(valid_labels, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "\n",
        "model_g = nn.Linear(Xtr.shape[1], 1).to(device)\n",
        "opt_g = torch.optim.Adam(model_g.parameters(), lr=1e-3)\n",
        "crit = nn.BCEWithLogitsLoss()\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "for epoch in range(epochs):\n",
        "    model_g.train()\n",
        "    total_loss = 0.0\n",
        "    for start in tqdm(range(0, Xtr.shape[0], batch_size), desc=f\"Gemma epoch {epoch+1}\"):\n",
        "        end = start + batch_size\n",
        "        xb = Xtr[start:end]\n",
        "        yb = y_train_t[start:end]\n",
        "        logits = model_g(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        opt_g.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_g.step()\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "model_g.eval()\n",
        "with torch.no_grad():\n",
        "    val_logits = model_g(Xva)\n",
        "    val_prob = torch.sigmoid(val_logits)\n",
        "    val_pred = (val_prob >= 0.5).float()\n",
        "    acc_g = float((val_pred.eq(y_valid_t)).float().mean().item())\n",
        "print(\"Gemma val acc:\", round(acc_g, 3))\n",
        "\n",
        "# отчёт и матрица ошибок (Gemma)\n",
        "y_true = y_valid_t.detach().cpu().numpy().ravel().astype(int)\n",
        "y_hat = val_pred.detach().cpu().numpy().ravel().astype(int)\n",
        "print(\"Gemma confusion matrix:\\n\", confusion_matrix(y_true, y_hat))\n",
        "print(\"Gemma classification report:\\n\", classification_report(y_true, y_hat, digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "b2b",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
